{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install spikingjelly\n!pip install torchsummary\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torchvision\nfrom spikingjelly.clock_driven import neuron, encoding, functional , surrogate, layer\nimport torch\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt \nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:25:07.638052Z","iopub.execute_input":"2022-02-22T19:25:07.638954Z","iopub.status.idle":"2022-02-22T19:25:22.391438Z","shell.execute_reply.started":"2022-02-22T19:25:07.638916Z","shell.execute_reply":"2022-02-22T19:25:22.390537Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### variables","metadata":{}},{"cell_type":"code","source":"device = 'cuda:0'\ndataset_dir = './'\nlog_dir = './'\nmodel_output_dir = './'\nbatch_size = 32\nlr = 1e-3\nT = 100\ntau = 2.0\ntrain_epoch = 15\nwriter = SummaryWriter(log_dir)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:25:22.394115Z","iopub.execute_input":"2022-02-22T19:25:22.394398Z","iopub.status.idle":"2022-02-22T19:25:26.336333Z","shell.execute_reply.started":"2022-02-22T19:25:22.394361Z","shell.execute_reply":"2022-02-22T19:25:26.335587Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### loading dataset","metadata":{}},{"cell_type":"code","source":"train_dataset = torchvision.datasets.MNIST(\n        root=dataset_dir,\n        train=True,\n        transform=torchvision.transforms.ToTensor(),\n        download=True\n)\ntest_dataset = torchvision.datasets.MNIST(\n        root=dataset_dir,\n        train=False,\n        transform=torchvision.transforms.ToTensor(),\n        download=True\n)\n\ntrain_data_loader = data.DataLoader(\n        dataset=train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n)\ntest_data_loader = data.DataLoader(\n        dataset=test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=False\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:25:26.339122Z","iopub.execute_input":"2022-02-22T19:25:26.339561Z","iopub.status.idle":"2022-02-22T19:25:28.456333Z","shell.execute_reply.started":"2022-02-22T19:25:26.339520Z","shell.execute_reply":"2022-02-22T19:25:28.454826Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### define model","metadata":{}},{"cell_type":"code","source":"net = nn.Sequential(\n        nn.Conv2d(1,16,kernel_size=3, padding=1, bias=False),\n        nn.BatchNorm2d(16),\n        nn.MaxPool2d(2, 2) ,\n        neuron.IFNode(surrogate_function=surrogate.ATan()),\n        nn.Conv2d(16,8 , kernel_size=3, padding=1, bias=False),\n        nn.MaxPool2d(2, 2) ,\n        neuron.IFNode(surrogate_function=surrogate.ATan()),\n        nn.Flatten(),\n        nn.Linear(392,10,bias=False),\n        neuron.LIFNode(tau=tau)\n)\nnet = net.to(device)\n    # optimizer\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    #define encoder\nencoder = encoding.PoissonEncoder()\ntrain_times = 0\nmax_test_accuracy = 0\n#mmary(net, input_size=(1,28,28))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:28:58.871085Z","iopub.execute_input":"2022-02-22T19:28:58.871871Z","iopub.status.idle":"2022-02-22T19:28:58.882314Z","shell.execute_reply.started":"2022-02-22T19:28:58.871816Z","shell.execute_reply":"2022-02-22T19:28:58.881594Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### training","metadata":{}},{"cell_type":"code","source":"test_accs = []\ntrain_accs = []\n\nfor epoch in range(train_epoch):\n        print(\"Epoch {}:\".format(epoch))\n        train_correct_sum = 0\n        train_sum = 0\n        net.train()\n        for img, label in tqdm(train_data_loader):\n            img = img.to(device)\n            label = label.to(device)\n            label_one_hot = F.one_hot(label, 10).float()\n\n            optimizer.zero_grad()\n\n            for t in range(T):\n                if t == 0:\n                    # encode and train one step\n                    out_spikes_counter = net(encoder(img).float())\n\n                else:\n                    out_spikes_counter += net(encoder(img).float())\n            #check the spikes frequency \n            out_spikes_counter_frequency = out_spikes_counter / T\n            # calculate the mse i think mse is better then cross entropy\n            loss = F.mse_loss(out_spikes_counter_frequency, label_one_hot)\n            # backward probagation \n            loss.backward()\n            optimizer.step()\n            functional.reset_net(net)\n            #metrics calulations\n            train_correct_sum += (out_spikes_counter_frequency.max(1)[1] == label.to(device)).float().sum().item()\n            train_sum += label.numel()\n\n            train_batch_accuracy = (out_spikes_counter_frequency.max(1)[1] == label.to(device)).float().mean().item()\n            writer.add_scalar('train_batch_accuracy', train_batch_accuracy, train_times)\n            train_accs.append(train_batch_accuracy)\n\n            train_times += 1\n        # the mean of the accuracy \n        train_accuracy = train_correct_sum / train_sum\n        net.eval()\n        with torch.no_grad():\n            test_correct_sum = 0\n            test_sum = 0\n            for img, label in tqdm(test_data_loader):\n                img = img.to(device)\n                for t in range(T):\n                    if t == 0:\n                        out_spikes_counter = net(encoder(img).float())\n                    else:\n                        out_spikes_counter += net(encoder(img).float())\n\n                test_correct_sum += (out_spikes_counter.max(1)[1] == label.to(device)).float().sum().item()\n                test_sum += label.numel()\n                functional.reset_net(net)\n            test_accuracy = test_correct_sum / test_sum\n            writer.add_scalar('test_accuracy', test_accuracy, epoch)\n            test_accs.append(test_accuracy)\n            .test_accuracy = max(max_test_accuracy, test_accuracy)\n        print(\"train_acc = {}, test_acc={}, max_test_acc={}, train_times={}\".format( train_accuracy, test_accuracy, max_test_accuracy, train_times))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:29:05.619203Z","iopub.execute_input":"2022-02-22T19:29:05.619504Z","iopub.status.idle":"2022-02-22T20:34:04.644056Z","shell.execute_reply.started":"2022-02-22T19:29:05.619464Z","shell.execute_reply":"2022-02-22T20:34:04.641509Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"torch.save(net, model_output_dir + \"/scnn_mnist.ckpt\")","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:34:11.470116Z","iopub.execute_input":"2022-02-22T20:34:11.470387Z","iopub.status.idle":"2022-02-22T20:34:11.485364Z","shell.execute_reply.started":"2022-02-22T20:34:11.470355Z","shell.execute_reply":"2022-02-22T20:34:11.484298Z"},"trusted":true},"execution_count":10,"outputs":[]}]}